================================================================================
GATEWAYZ BETA - CHAT SYSTEM PERFORMANCE BOTTLENECK ANALYSIS
================================================================================

PROJECT: Gatewayz Beta (beta.gatewayz.ai)
ANALYSIS DATE: 2025-11-15
ISSUE: Chat responses taking 60+ seconds or hanging indefinitely

================================================================================
TOP 10 PERFORMANCE BOTTLENECKS (Priority Order)
================================================================================

1. CLIENT-SIDE 2-MINUTE TIMEOUT [CRITICAL]
   Location: src/lib/streaming.ts:105
   Issue: 120s total timeout aborts long-running requests prematurely
   Impact: PRIMARY CAUSE of 60+ second hangs
   Fix: Increase to 300-600 seconds (5-10 minutes)

2. NO BACKEND API FETCH TIMEOUT [HIGH]
   Location: src/app/api/chat/completions/route.ts:87-94
   Issue: Fetch can hang indefinitely with no timeout configured
   Impact: Silent hangs, client timeout is only protection
   Fix: Add AbortSignal.timeout(30000) to fetch options

3. EXPONENTIAL RETRY STACKING [HIGH]
   Location: src/lib/streaming.ts:138-150
   Issue: Retries consume 60+ seconds cumulatively (2s+4s+8s+16s+32s = 62s)
   Impact: Delays accumulate across server and client retries
   Fix: Reduce backoff to 500ms-5s, limit to 3 attempts

4. SYNCHRONOUS MESSAGE SAVING BEFORE STREAMING [MEDIUM-HIGH]
   Location: src/app/chat/page.tsx:2348-2388
   Issue: Blocks stream start for 0-30s while saving user message
   Impact: Adds 5-30s visible delay before response appears
   Fix: Move to async call after streaming starts

5. UNOPTIMIZED STREAMING (EXCESSIVE LOGGING) [MEDIUM]
   Location: src/lib/streaming.ts (45 devLog calls)
   Issue: Logs entire response objects, JSON.stringify() on hot path
   Impact: 5-10% CPU overhead on streaming
   Fix: Remove expensive logging, use conditional checks only

6. NO CONNECTION POOLING/KEEP-ALIVE [MEDIUM]
   Location: src/app/api/chat/completions/route.ts & chat-history.ts
   Issue: Each request creates new TCP/SSL connection
   Impact: 100-500ms overhead per request
   Fix: Add Connection: keep-alive headers, consider Node.js runtime

7. EDGE RUNTIME COLD STARTS [MEDIUM]
   Location: src/app/api/chat/completions/route.ts:6
   Issue: Fresh container per request spike, no connection persistence
   Impact: 50-200ms delay on traffic spikes
   Fix: Consider Node.js runtime for persistent containers

8. RATE LIMIT RETRY RECURSION [MEDIUM]
   Location: src/lib/streaming.ts:245-284
   Issue: Recursive generator can nest deeply, no max total retry time
   Impact: Rate-limited models can add 30+ seconds
   Fix: Implement iterative retry, cap total wait time

9. SESSION API BLOCKING [MEDIUM]
   Location: src/lib/chat-history.ts:87-118
   Issue: 30s timeout per call, but no total timeout across operations
   Impact: Session operations can pile up
   Fix: Add total operation timeout, implement circuit breaker

10. SESSION CREATION DELAY [LOW-MEDIUM]
    Location: src/app/chat/page.tsx:2209-2233
    Issue: First message waits for session creation (1-5s)
    Impact: First message noticeably slower than subsequent ones
    Fix: Create session in parallel with message composition

================================================================================
CRITICAL TIMEOUT CONFIGURATIONS
================================================================================

CLIENT-SIDE TIMEOUTS:
  - Streaming response: 120s (line 105, streaming.ts) ← TOO SHORT
  - Model select fetch: 10s (line 203, model-select.tsx)
  - ChatHistoryAPI default: 30s (line 87, chat-history.ts)
  - Model prefetch: 10s (line 511, model-select.tsx)
  - Model detail fetch: 5s (models/[...name]/page.tsx)

BACKEND TIMEOUTS:
  - Edge runtime fetch: NONE (infinite!) ← CRITICAL GAP
  - Retry backoff: 2s→4s→8s = 14s+ per failure
  - Rate limit retry: Unbounded

STREAMING RETRIES:
  - Client: 5 attempts, exponential backoff (2s, 4s, 8s, 16s, 32s = 62s total wait)
  - Edge: 3 attempts, exponential backoff (2s, 4s, 8s = 14s total wait)
  - Combined: Can reach 76+ seconds just waiting for retries

================================================================================
CURRENT REQUEST FLOW DIAGRAM
================================================================================

User types "Hello" and clicks Send
            ↓
    ┌─────────────────────────────┐
    │ handleSendMessage()         │ (chat/page.tsx:2146)
    └─────────────────────────────┘
            ↓
    ┌─────────────────────────────┐
    │ Save user message to API    │ ← SYNCHRONOUS BLOCK (0-30s)
    │ (ChatHistoryAPI.saveMessage)│   (chat/page.tsx:2348-2388)
    └─────────────────────────────┘
            ↓
    ┌─────────────────────────────┐
    │ POST /api/chat/completions  │ (Edge Runtime)
    │ (streaming.ts calls this)   │
    └─────────────────────────────┘
            ↓
    ┌─────────────────────────────┐
    │ Edge forwards to backend:    │
    │ POST api.gatewayz.ai:v1/     │ ← NO TIMEOUT HERE!
    │      chat/completions       │
    │                             │
    │ Retry logic: 3x attempts    │
    │ with backoff (2s,4s,8s)     │
    └─────────────────────────────┘
            ↓
    ┌─────────────────────────────┐
    │ Client receives streaming   │
    │ response (SSE)              │
    │                             │
    │ 120-second timeout          │ ← TOO SHORT for long models
    │ 5x retry attempts           │
    │ with backoff (2s,4s,8s,... )│
    │ = 62s wait on retries alone │
    └─────────────────────────────┘
            ↓
    ┌─────────────────────────────┐
    │ Process streaming chunks    │
    │ (45+ debug logs on hot path)│ ← CPU OVERHEAD
    └─────────────────────────────┘
            ↓
    ┌─────────────────────────────┐
    │ Save assistant response     │ ASYNC (after streaming ends)
    │ (fire and forget)           │
    └─────────────────────────────┘

PROBLEM: If save message takes 10s, streaming only has 110s left of 120s timeout.
         With 62s of retry waits, only 48s remains for actual streaming.

================================================================================
QUICK FIX CHECKLIST (Do These First)
================================================================================

Priority 1 (Critical - 30 minutes):
  □ Increase streaming timeout from 120s to 300s (5 minutes)
    File: src/lib/streaming.ts:105
    Change: setTimeout(() => controller.abort(), 120000)
    To: setTimeout(() => controller.abort(), 300000)

  □ Add timeout to backend fetch call
    File: src/app/api/chat/completions/route.ts:87-94
    Add to fetch options: signal: AbortSignal.timeout(30000)

  □ Move message save to async (don't await before streaming)
    File: src/app/chat/page.tsx:2348-2388
    Change: await chatAPI.saveMessage()
    To: chatAPI.saveMessage().catch(console.error)

Priority 2 (High - 1-2 hours):
  □ Reduce retry wait times
    File: src/lib/streaming.ts:139
    Change: 2000 * Math.pow(2, retryCount) to 500 * Math.pow(2, retryCount)

  □ Comment out expensive debug logging
    File: src/lib/streaming.ts:383 (JSON.stringify)
    Add: // in production to skip object serialization

  □ Add keep-alive header
    File: src/app/api/chat/completions/route.ts:89-91
    Add: 'Connection': 'keep-alive'

================================================================================
PERFORMANCE IMPACT ANALYSIS
================================================================================

Current Performance (Baseline):
  - Slow response: 60-120+ seconds
  - User sees: "Request timed out after 2 minutes"
  - Root cause: Client timeout + retry delays

With Quick Fix Only (30 min work):
  - Expected: 40-80 second responses
  - Improvement: 20-40 seconds faster
  - Reason: Timeout extension + async save + reduced wait

With All Optimizations (4-6 hours work):
  - Expected: 15-45 second responses
  - Improvement: 60+ seconds faster
  - Reason: Connection pooling + Node.js runtime + optimized logging

================================================================================
FILES TO REVIEW & FIX
================================================================================

Critical (Fix First):
  1. src/lib/streaming.ts
     - Lines 103-105: Increase timeout
     - Lines 137-150: Fix retry backoff
     - Lines 383-585: Remove expensive logging

  2. src/app/api/chat/completions/route.ts
     - Lines 6: Consider runtime selection
     - Lines 87-94: Add fetch timeout
     - Lines 89-91: Add keep-alive header

  3. src/app/chat/page.tsx
     - Lines 2146-2200: handleSendMessage function
     - Lines 2348-2388: Move message save to async

Important (Fix Second):
  4. src/lib/chat-history.ts
     - Lines 87-118: Review timeout logic
     - Lines 279-330: Message save timeout

  5. src/components/chat/model-select.tsx
     - Lines 203-206: 10s timeout for model fetch

Optional (Optimization):
  6. src/app/models/page.tsx
  7. src/app/models/[...name]/page.tsx

================================================================================
SUCCESS METRICS
================================================================================

Before Optimization:
  - P50 response time: 45-60 seconds
  - P95 response time: 80-120 seconds
  - Error rate: 10-20% (timeout errors)
  - User satisfaction: Low (hang perception)

Target After Quick Fix:
  - P50 response time: 20-30 seconds (first message)
  - P95 response time: 40-60 seconds
  - Error rate: <5%
  - User satisfaction: High (responsive feel)

================================================================================
NOTES
================================================================================

1. Streaming IS working correctly - it's just too slow due to timeouts
2. The Edge runtime is appropriate but needs fetch timeout
3. Retry logic is reasonable but timing is too aggressive
4. Message saving should not block user-visible response
5. Debug logging in production adds unnecessary CPU usage
6. Connection reuse would help significantly

For more details, see: CHAT_PERFORMANCE_ANALYSIS.md

================================================================================
