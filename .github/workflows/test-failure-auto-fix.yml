name: Codex Test Fixer

on:
  workflow_call:
    inputs:
      test_output_artifact:
        description: 'Name of artifact containing test output'
        required: true
        type: string

permissions:
  id-token: write
  contents: write
  pull-requests: write
  checks: write
  issues: write

jobs:
  auto-fix:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Download test failures
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.test_output_artifact }}

      - name: Run Codex Auto-Fix
        uses: anthropics/claude-code-action@8a1c4371755898f67cd97006ba7c97702d5fc4bf  # v1.0.16
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          use_commit_signing: false
          claude_args: "--max-turns 20"
          prompt: |
            The CI tests are failing. Your task is to analyze the test failures and fix the bugs.

            **IMPORTANT**: This codebase is a FastAPI AI gateway with 43,491 lines of Python code:
            - Main entry point: src/main.py
            - Routes: src/routes/ (29 modules)
            - Services: src/services/ (52 modules)
            - Database: src/db/ (16 modules)
            - Schemas: src/schemas/ (13 modules)

            **Failed Test Output** (read test_failures.txt):
            - File: test_failures.txt
            - Contains: Full pytest output with failed tests from sharded test runs

            **Your Mission:**
            1. **Analyze** the test failures from test_failures.txt
            2. **Identify** the root causes in the source code
            3. **Implement** fixes to make tests pass
            4. **Run verification** to confirm fixes work

            **Fix Implementation Guide:**

            1. **Read test output** to understand what failed:
               - Extract the failing test name (e.g., "tests/routes/test_chat.py::test_chat_completions")
               - Read the error message - note that tests may be from sharded runs
               - Look at the assertion or exception details
               - Note any timeout or async issues

            2. **Locate the bug** in source code:
               - Look at the test file to understand what it's testing
               - Find the corresponding implementation in src/
               - Identify the bug or missing code
               - Check for async/await issues (common in FastAPI)
               - Look for dependency injection problems

            3. **Fix the bug**:
               - Apply the minimal fix needed to make the test pass
               - Don't add unnecessary features
               - Maintain code quality and style
               - Ensure fixes work across all 4 test shards

            4. **Common bug patterns to look for in this codebase:**
               - Missing error handling in async functions
               - Incorrect return values or response schemas
               - Type mismatches (especially with Pydantic models)
               - Missing imports or attributes
               - Logic errors in provider routing
               - Missing validation on user input
               - Database query issues
               - Missing mock configurations in tests
               - Async context manager issues

            5. **Priority order for fixes:**
               - Import errors (easiest)
               - Type errors (medium)
               - Async/await issues (important for FastAPI)
               - Logic errors (harder)
               - Integration issues (hardest)

            **Codebase-Specific Notes:**
            - All API endpoints use FastAPI with async/await
            - Database queries use Supabase PostgREST API
            - Tests use pytest with pytest-asyncio
            - Use src/schemas/ for request/response models
            - Check src/security/deps.py for auth dependencies
            - Provider clients in src/services/\*_client.py

            **After Fixing:**
            - Verify: Run `pytest tests/ -v --tb=short -x` locally if possible
            - All fixes must pass both unit and integration tests
            - Commit messages should explain the fix clearly

            **DO:**
            - Read test files to understand requirements
            - Check existing code patterns for consistency
            - Add proper error handling
            - Include type hints where appropriate
            - Follow the codebase's style (100 char lines, Black formatted)
            - Use async/await correctly throughout

            **DON'T:**
            - Add new features beyond fixing the test
            - Break other tests with your changes
            - Leave debugging code or print statements
            - Ignore error cases
            - Create unnecessary intermediate files
            - Remove existing functionality

      - name: Set git config for commits
        run: |
          git config user.name "Codex Auto-Fix Bot"
          git config user.email "codex-autofix@anthropic.com"

      - name: Commit fixes if Codex made changes
        id: commit
        run: |
          set +e

          # Check if there are changes
          if [ -z "$(git status --porcelain)" ]; then
            echo "No changes to commit"
            echo "changes_made=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "changes_made=true" >> $GITHUB_OUTPUT

          # Stage all changes
          git add -A

          # Create commit message
          FAILED_TESTS=$(git diff --cached --name-only | grep "test" | head -5 || echo "tests")
          git commit -m "fix: auto-fix failing tests

Auto-fixed failing tests using Codex analysis.

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Codex <noreply@anthropic.com>"

          echo "‚úÖ Committed auto-fixes"

      - name: Re-run tests to verify fixes
        if: steps.commit.outputs.changes_made == 'true'
        id: verify
        run: |
          set +e

          python -m pip install --upgrade pip
          pip install -q -r requirements.txt pytest pytest-asyncio pytest-timeout

          # Run tests again
          pytest tests/ -v --tb=short -x 2>&1 | tee /tmp/verify_output.txt
          VERIFY_EXIT_CODE=$?

          if [ $VERIFY_EXIT_CODE -eq 0 ]; then
            echo "Tests now PASS! ‚úÖ"
            echo "verify_success=true" >> $GITHUB_OUTPUT
          else
            echo "Tests still failing, may need additional fixes"
            echo "verify_success=false" >> $GITHUB_OUTPUT

            # Save for next iteration
            cat /tmp/verify_output.txt > test_failures.txt
          fi

          exit 0  # Don't fail this job

      - name: Push fixes to branch
        if: steps.commit.outputs.changes_made == 'true'
        run: |
          # Only push if we have a branch to push to
          if [ "${{ github.event_name }}" == "push" ] || [ "${{ github.event_name }}" == "pull_request" ]; then
            git push origin HEAD:${{ github.head_ref || github.ref_name }}
          fi

      - name: Create issue if fixes still needed
        if: steps.verify.outputs.verify_success == 'false' && github.event_name == 'push'
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea
        with:
          script: |
            try {
              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'üîß Tests Still Failing - Needs Review',
                body: `Codex attempted to auto-fix failing tests, but some are still failing.

**Workflow Run:** ${context.runId}

**Next Steps:**
1. Review the workflow run logs
2. Check the auto-fixes that were committed
3. Manually debug remaining failures
4. Re-run the workflow or commit fixes

**Artifacts:**
- Test failure details available in workflow artifacts`,
                labels: ['bug', 'auto-fix', 'needs-review'],
              });

              console.log(`Created issue #${issue.data.number}`);
            } catch (error) {
              console.error('Failed to create issue:', error);
            }

      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea
        with:
          script: |
            const verify_success = '${{ steps.verify.outputs.verify_success }}' === 'true';
            const changes_made = '${{ steps.commit.outputs.changes_made }}' === 'true';

            let comment = '## üîß Codex Auto-Fix Results\n\n';

            if (!changes_made) {
              comment += '‚úÖ Tests passed on first run - no fixes needed!\n';
            } else if (verify_success) {
              comment += '‚úÖ **All tests now pass!**\n\n';
              comment += 'Codex successfully identified and fixed the failing tests.\n\n';
              comment += 'Fixes have been committed and pushed to this PR.\n';
            } else {
              comment += '‚ö†Ô∏è **Some tests still failing after auto-fix**\n\n';
              comment += 'Codex attempted fixes but additional debugging may be needed.\n\n';
              comment += 'Check the workflow run for details: ' + context.runId + '\n';
            }

            try {
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
              console.log('PR comment posted successfully');
            } catch (error) {
              console.error('Failed to post PR comment:', error);
            }
